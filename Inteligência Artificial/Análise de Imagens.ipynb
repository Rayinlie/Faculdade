{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs disponíveis: ['/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "def get_available_gpus():\n",
    "    \"\"\" Retorna uma lista dos identificadores das GPUs disponíveis. \"\"\"\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "available_gpus = get_available_gpus()\n",
    "if available_gpus:\n",
    "    print(\"GPUs disponíveis:\", available_gpus)\n",
    "else:\n",
    "    print(\"Nenhuma GPU disponível.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCMercedDataset(Sequence):\n",
    "    def __init__(self, root_dir, image_size=(256, 256)):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_size = image_size\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for label_dir in os.listdir(root_dir):\n",
    "            class_dir = os.path.join(root_dir, label_dir)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for img_file in os.listdir(class_dir):\n",
    "                    if img_file.endswith(\".tif\") or img_file.endswith(\".jpg\"):\n",
    "                        self.images.append(os.path.join(class_dir, img_file))\n",
    "                        self.labels.append(label_dir)\n",
    "        self.labels = [self.labels.index(l) for l in self.labels]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).resize(self.image_size).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        image = np.array(image) / 255.0\n",
    "        return image, label\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        temp = list(zip(self.images, self.labels))\n",
    "        np.random.shuffle(temp)\n",
    "        self.images, self.labels = zip(*temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m UCMercedDataset(root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData/UCM_captions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[dataset[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset))])  \u001b[38;5;66;03m# Extrai todas as imagens e labels\u001b[39;00m\n\u001b[0;32m      3\u001b[0m images_label_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(images, labels))  \u001b[38;5;66;03m# Cria um dicionário com imagens e labels\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Divisão de dados\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m UCMercedDataset(root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData/UCM_captions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset))])  \u001b[38;5;66;03m# Extrai todas as imagens e labels\u001b[39;00m\n\u001b[0;32m      3\u001b[0m images_label_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(images, labels))  \u001b[38;5;66;03m# Cria um dicionário com imagens e labels\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Divisão de dados\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 24\u001b[0m, in \u001b[0;36mUCMercedDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     22\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mresize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m---> 24\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = UCMercedDataset(root_dir='Data/UCM_captions')\n",
    "images, labels = zip(*[dataset[i] for i in range(len(dataset))])  # Extrai todas as imagens e labels\n",
    "images_label_dict = dict(zip(images, labels))  # Cria um dicionário com imagens e labels\n",
    "# Divisão de dados\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    images, labels, test_size=0.20, random_state=0\n",
    ")\n",
    "\n",
    "np.random.shuffle(images_label_dict)\n",
    "\n",
    "# Criar objetos Dataset do TensorFlow\n",
    "train_data = tf.data.Dataset.from_tensor_slices((list(train_images), list(train_labels))).batch(32)\n",
    "test_data = tf.data.Dataset.from_tensor_slices((list(test_images), list(test_labels))).batch(32)\n",
    "\n",
    "print(\"Tamanho do conjunto de treino:\", len(train_images))\n",
    "print(\"Tamanho do conjunto de teste:\", len(test_images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 256, 256, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 256, 256, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 128, 128, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 128, 128, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 128, 128, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 64, 64, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 64, 64, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 64, 64, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 64, 64, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 32, 32, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 32, 32, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 16, 16, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 21)                10773     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,725,461\n",
      "Trainable params: 14,725,461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Carrega o modelo VGG16 sem pesos pré-treinados\n",
    "base_model = VGG16(weights=None, include_top=False, input_shape=(256, 256, 3))\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dense(21, activation='softmax')(x)  # 21 classes\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório para salvar os checkpoints do modelo\n",
    "checkpoint_dir = 'model_checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_filepath = os.path.join(checkpoint_dir, 'best_model.h5')\n",
    "\n",
    "# Callback para salvar o modelo com a melhor acurácia de validação\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_best_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Callback para parar o treinamento se a acurácia de validação não melhorar após 5 épocas\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='max',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Callback para TensorBoard\n",
    "tensorboard_callback = TensorBoard(\n",
    "    log_dir='logs',\n",
    "    histogram_freq=1,  # Frequência (em épocas) para calcular histogramas de ativações e pesos\n",
    "    write_graph=True\n",
    ")\n",
    "\n",
    "# Lista de callbacks\n",
    "callbacks_list = [model_checkpoint, early_stopping, tensorboard_callback]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.5776 - accuracy: 0.9810\n",
      "Epoch 1: val_accuracy improved from -inf to 1.00000, saving model to model_checkpoints\\best_model.h5\n",
      "53/53 [==============================] - 27s 431ms/step - loss: 0.5776 - accuracy: 0.9810 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 2/50\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 2: val_accuracy did not improve from 1.00000\n",
      "53/53 [==============================] - 20s 386ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 3/50\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 3: val_accuracy did not improve from 1.00000\n",
      "53/53 [==============================] - 20s 381ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 4/50\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 4: val_accuracy did not improve from 1.00000\n",
      "53/53 [==============================] - 20s 386ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 5/50\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 5: val_accuracy did not improve from 1.00000\n",
      "53/53 [==============================] - 20s 382ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 6: val_accuracy did not improve from 1.00000\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "53/53 [==============================] - 20s 378ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 6: early stopping\n",
      "Treinamento concluído\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinamento\n",
    "history = model.fit(\n",
    "    train_data,  # passando diretamente o dataset de treino\n",
    "    validation_data=test_data,  # passando diretamente o dataset de teste\n",
    "    epochs=3,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1  # Para obter detalhes do treinamento em cada época\n",
    ")\n",
    "\n",
    "print(\"Treinamento concluído\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
